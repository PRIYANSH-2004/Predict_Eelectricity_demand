{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# How to use this file?\n",
        "1. Make sure that lgb_model_summarized.pkl and xgb_model_summarized.pkl are in the same directory as the notebook.\n",
        "2. Just hit run all to run all the code blocks.\n",
        "3. When prompted insert the path of the folder contatining the location of the folder containing all the input CSVs.\n",
        "4. The predictions will be saved to demand_predictions_ensemble.csv and processed_weather_data.csv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install jpholiday\n",
        "!pip install joblib\n",
        "!pip install xgboost\n",
        "!pip install lightgbm\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import jpholiday\n",
        "from typing import Dict\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class WeatherDataPipeline:\n",
        "    def __init__(self, city_weights: Dict[str, float]):\n",
        "        self.city_weights = city_weights\n",
        "        self.scaler = MinMaxScaler()\n",
        "    \n",
        "    def translate_wind_direction(self, direction_chinese: str) -> float:\n",
        "        translations = {\n",
        "            '南': 180, '南南西': 202.5, '南南東': 157.5, '西': 270, '北西': 315,\n",
        "            '南西': 225, '西北西': 292.5, '西南西': 247.5, '南東': 135,\n",
        "            '北北西': 337.5, '北東': 45, '東南東': 112.5, '東': 90, '北': 0,\n",
        "            '北北東': 22.5, '東北東': 67.5, '静穏': 0, '×': -1\n",
        "        }\n",
        "        return translations.get(direction_chinese, -1)\n",
        "    \n",
        "    def process_single_file(self, file_path: str, city_name: str) -> pd.DataFrame:\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, parse_dates=['datetime'])\n",
        "            df['wind_direction'] = df['wind_direction'].apply(self.translate_wind_direction)\n",
        "            df.rename(columns={col: f\"{col}_{city_name}\" for col in df.columns if col != 'datetime'}, inplace=True)\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {city_name} file: {str(e)}\")\n",
        "            return None\n",
        "    \n",
        "    def weighted_mode(self, series: pd.Series) -> float:\n",
        "        valid_data = series[series != -1]\n",
        "        if valid_data.empty:\n",
        "            return -1\n",
        "        unique_values, counts = np.unique(valid_data, return_counts=True)\n",
        "        return unique_values[np.argmax(counts)]\n",
        "    \n",
        "    def merge_city_data(self, city_dfs: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
        "        valid_dfs = {city: df for city, df in city_dfs.items() if df is not None}\n",
        "        if not valid_dfs:\n",
        "            raise ValueError(\"No valid DataFrames to merge\")\n",
        "        \n",
        "        merged_df = list(valid_dfs.values())[0]\n",
        "        for city, df in list(valid_dfs.items())[1:]:\n",
        "            merged_df = pd.merge(merged_df, df, on='datetime', how='outer')\n",
        "        \n",
        "        merged_df.sort_values('datetime', inplace=True)\n",
        "        \n",
        "        for col in merged_df.columns:\n",
        "            if col != 'datetime':\n",
        "                merged_df[col] = merged_df[col].fillna(method='ffill', limit=1)\n",
        "                merged_df[col] = merged_df[col].fillna(method='bfill', limit=1)\n",
        "        \n",
        "        weather_cols = [col for col in merged_df.columns if col != 'datetime']\n",
        "        merged_df.dropna(subset=weather_cols, how='all', inplace=True)\n",
        "        \n",
        "        return merged_df\n",
        "    \n",
        "    def aggregate_features(self, merged_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        result_df = pd.DataFrame({'datetime': merged_df['datetime'].unique()})\n",
        "        features = ['precipitation', 'temperature', 'dew_point_temperature', 'humidity', 'wind_speed', 'snowfall', 'wind_direction']\n",
        "        total_weight = sum(self.city_weights.values())\n",
        "        \n",
        "        for feature in features:\n",
        "            feature_cols = [col for col in merged_df.columns if col.startswith(feature)]\n",
        "            \n",
        "            if feature == 'wind_direction':\n",
        "                result_df[feature] = merged_df[feature_cols].apply(self.weighted_mode, axis=1)\n",
        "            else:\n",
        "                weighted_sum = pd.Series(0, index=merged_df.index)\n",
        "                weights_sum = 0\n",
        "                \n",
        "                for col in feature_cols:\n",
        "                    city = col.split('_')[-1]\n",
        "                    if city in self.city_weights:\n",
        "                        weight = self.city_weights[city]\n",
        "                        weighted_sum += merged_df[col].fillna(0) * weight\n",
        "                        weights_sum += weight\n",
        "                \n",
        "                result_df[feature] = weighted_sum / weights_sum if weights_sum > 0 else 0\n",
        "        \n",
        "        return result_df\n",
        "    \n",
        "    def add_time_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        df['hour'] = df['datetime'].dt.hour\n",
        "        df['day_of_week'] = df['datetime'].dt.dayofweek\n",
        "        df['month'] = df['datetime'].dt.month\n",
        "        df['is_holiday'] = df['datetime'].apply(lambda x: jpholiday.is_holiday(x))\n",
        "        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
        "        return df\n",
        "    \n",
        "    def scale_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        numerical_columns = df.columns.difference(['datetime'])\n",
        "        df[numerical_columns] = self.scaler.fit_transform(df[numerical_columns])\n",
        "        return df\n",
        "    \n",
        "    def process_folder(self, folder_path: str) -> pd.DataFrame:\n",
        "        city_dfs = {}\n",
        "        \n",
        "        for file_name in os.listdir(folder_path):\n",
        "            if file_name.endswith('.csv'):\n",
        "                city_name = os.path.splitext(file_name)[0].lower()\n",
        "                file_path = os.path.join(folder_path, file_name)\n",
        "                city_dfs[city_name] = self.process_single_file(file_path, city_name)\n",
        "        \n",
        "        merged_df = self.merge_city_data(city_dfs)\n",
        "        final_df = self.aggregate_features(merged_df)\n",
        "        final_df = self.add_time_features(final_df)\n",
        "        final_df = self.scale_features(final_df)\n",
        "        \n",
        "        return final_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# city weights on the basis of diffrent factors such as population, industries, urban, rural etc...\n",
        "city_weights = {\n",
        "    'osaka': 1.00, 'kobe': 0.85, 'kyoto': 0.75, 'wakayama': 0.55,\n",
        "    'hikone': 0.35, 'toyooka': 0.25, 'shionomisaki': 0.10, 'nara': 0.45\n",
        "}\n",
        "\n",
        "folder_path = input(\"Enter path of folder with all input CSVs:\")\n",
        "\n",
        "pipeline = WeatherDataPipeline(city_weights)\n",
        "processed_data = pipeline.process_folder(folder_path)\n",
        "processed_data.to_csv('processed_weather_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ensemble predictions saved successfully to demand_predictions_ensemble.csv\n"
          ]
        }
      ],
      "source": [
        "# Load trained models\n",
        "xgb_model = joblib.load(\"xgb_model_summarized.pkl\")\n",
        "lgb_model = joblib.load(\"lgb_model_summarized.pkl\")\n",
        "\n",
        "# Load dataset\n",
        "processed_data = pd.read_csv(\"processed_weather_data.csv\", parse_dates=[\"datetime\"])\n",
        "\n",
        "# Extract features\n",
        "features = [col for col in processed_data.columns if col != \"datetime\"]\n",
        "\n",
        "# Generate predictions from both models\n",
        "xgb_pred = xgb_model.predict(processed_data[features])\n",
        "lgb_pred = lgb_model.predict(processed_data[features])\n",
        "\n",
        "# Ensemble using weighted averaging\n",
        "w_xgb = 0.5  # Adjust weights based on model performance\n",
        "w_lgb = 0.5\n",
        "ensemble_pred = (w_xgb * xgb_pred) + (w_lgb * lgb_pred)\n",
        "\n",
        "# Store predictions\n",
        "processed_data[\"demand\"] = ensemble_pred\n",
        "\n",
        "# Save to CSV\n",
        "processed_data[[\"datetime\", \"demand\"]].to_csv(\"demand_predictions_ensemble.csv\", index=False)\n",
        "\n",
        "print(\"Ensemble predictions saved successfully to demand_predictions_ensemble.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
